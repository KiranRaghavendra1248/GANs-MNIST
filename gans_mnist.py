# -*- coding: utf-8 -*-
"""GANs MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Ko86cZb7TGo2LBo1DDKWQYa-inYw4DN
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torchvision
import torch.nn as nn
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
import matplotlib.pyplot as plt
torch.manual_seed(0)
# %load_ext tensorboard
from torch.utils.tensorboard import SummaryWriter

torch.__version__

train_set=torchvision.datasets.MNIST(root='./data/MNIST',
                                            train=True,
                                            transform=transforms.Compose([
                                            transforms.ToTensor()
                                            ]),
                                            download=True  
)

# Helper/Utility functions
def display_grid(images,num=25,size=(1,28,28)):
  images_=images.detach().cpu().reshape(-1,*size)
  grid=utils.make_grid(images_[:num],nrow=5)
  # print(grid.shape)
  return grid

def return_gen(input_dim,output_dim):
  temp=nn.Sequential(
      nn.Linear(input_dim,output_dim),
      nn.BatchNorm1d(output_dim),
      nn.LeakyReLU(0.2,inplace=True)
  )
  return temp

def return_disc(input_dim,output_dim):
  temp=nn.Sequential(
      nn.Linear(input_dim,output_dim),
      nn.LeakyReLU(0.2,inplace=True)
  )
  return temp

def noise(num,z_len,device='cuda'):
  return torch.randn((num,z_len),device=device)

# Define Generator and Discriminator classes
class Generator(nn.Module):
 
  def __init__(self,z_len,num_blocks=3,hidden_dim=128,img_dim=784):
    super(Generator,self).__init__()
    self.layer=nn.Sequential()
    self.layer.add_module('Block_1',return_gen(z_len,hidden_dim))
    i=1
    count=2
    while num_blocks:
      self.layer.add_module('Block_{0}'.format(count),return_gen(hidden_dim*i,hidden_dim*(2*i)))
      i*=2
      count+=1
      num_blocks-=1
    self.layer.add_module('Linear',nn.Linear(hidden_dim*i,img_dim))
    self.layer.add_module('Sigmoid',nn.Sigmoid())
    
  def forward(self,noise):
    return self.layer(noise)
 
class Discriminator(nn.Module):
 
  def __init__(self,img_dim=784):
    super(Discriminator,self).__init__()
    self.layer=nn.Sequential(
        return_disc(img_dim,512),
        return_disc(512,256),
        return_disc(256,128),
        nn.Linear(128,1)  # Not using activation as its already used inside BCE with logits loss function
    )
 
  def forward(self,imgs):
    return self.layer(imgs)



# Define losses for Generator and Discriminator
def generator_loss(criterion,fake_imgs,disc,device):
  fake_pred=disc(fake_imgs)
  fake_truth=torch.ones(fake_pred.shape,device=device)
 
  gen_loss=criterion(fake_pred,fake_truth)
  return gen_loss
 
def discriminator_loss(criterion,fake_imgs,real_imgs,disc,device):
  fake_pred=disc(fake_imgs)
  real_pred=disc(real_imgs)
 
  fake_truth=torch.zeros(fake_pred.shape,device=device)*0.1
  real_truth=torch.ones(real_pred.shape,device=device)*0.9
 
  real_loss=criterion(real_pred,real_truth)
  fake_loss=criterion(fake_pred,fake_truth)
  return (real_loss+fake_loss)/2

# Hyperparameters
criterion = nn.BCEWithLogitsLoss()
lr=0.0002
batch_size=128
epochs=200
display_step=100
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
z_len=64
 
 
data_loader=DataLoader(train_set,batch_size,shuffle=True)
 
gen = Generator(z_len,3).to(device)
gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)
disc = Discriminator().to(device) 
disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)
 
gen.train()
disc.train()
 
writer_real=SummaryWriter(f'runs/GANS_MNIST/real_new')
writer_fake=SummaryWriter(f'runs/GANS_MNIST/fake_new')
 
print(gen)
print(disc)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir runs

cur_step=0
for epoch in range(epochs):
  loop=tqdm(data_loader,total=len(data_loader),leave=False) 
  # leave=False if we want progress bar to disappear and start afresh
  for (real_imgs,_) in loop:
    cur_step+=1
    cur_batch_size=len(real_imgs)
 
    # Flatten out the images
    real_imgs=real_imgs.view(cur_batch_size,-1).to(device)
 
    # Make previous grads 0 for discriminator's optimizer
    disc_opt.zero_grad()
 
    # Make fake imgs 
    noise_tensor=noise(cur_batch_size,z_len,device)
    fake_imgs=gen(noise_tensor)
 
    # Get disc loss on real and fake imgs
    disc_loss=discriminator_loss(criterion,fake_imgs,real_imgs,disc,device)
 
    disc_loss.backward(retain_graph=True)
 
    disc_opt.step()
 
    
    # Make previous grads 0 for generator's optimizer
    gen_opt.zero_grad()
 
 
    # Get gen_loss
    gen_loss = generator_loss(criterion,fake_imgs,disc,device)
 
    gen_loss.backward(retain_graph=True)
 
    gen_opt.step()
 
    if cur_step%display_step==0:
      real_img_grid=display_grid(real_imgs)
      fake_img_grid=display_grid(fake_imgs)
      writer_real.add_image('MNIST Real',real_img_grid,global_step=cur_step)
      writer_fake.add_image('MNIST Fake',fake_img_grid,global_step=cur_step)
      writer_real.add_scalar('Generator Loss',gen_loss,global_step=cur_step)
      writer_real.add_scalar('Discriminator Loss',disc_loss,global_step=cur_step)

 
    # Update progress bar
    loop.set_description(f"Epoch:[{epoch}/{epochs}]")
    loop.set_postfix(Generator_loss=gen_loss.item(),Discriminator_loss=disc_loss.item())

